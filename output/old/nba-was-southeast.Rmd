---
title: "Analysis"
output: html_document
params:
  tweets: NULL
  filepath_tweets: NULL
  names: NULL
  name_main: NULL
  names_main: NULL
  augmented: TRUE
  augmented_colname: "name"
  download: FALSE
  download_method: c("timline", "search")
  screen_names: NULL
  tweets_min_download: 1000
  num_main_max: 6
  pal_main: NULL
  color_main: NULL
  colors_main: NULL
  tweet_cnt_min: 1000
  trim_time: FALSE
  dd_cnt_min: 7
  yyyy_cnt_min: 1
  mm_cnt_min: 12
  wday_cnt_min: 7
  hh_cnt_min: 2
  kinds_features: c("hashtag", "link")
  kinds_types: c("quote", "reply", "rt")
---


```{r validate_params}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

library("dplyr")
library("stringr")
library("ggplot2")
library("tidyr")
# library("lubridate")
# library("scales")
# library("tidytext")
# library("rtweet")
library("temisc")

filepath_functions <- file.path("functions-analyze.R")
filepath_functions <- normalizePath(filepath_functions, winslash = "/")
source(filepath_functions)
params_valid <- validate_params(params)
```





```{r process_params}
# Setup. ----
# NOTE: Unfortunately, it doesn't seem like get_timelines() and search_tweets2()
# really work as I initially intended. Some like of loop with get_timeline() and search_tweet()
# will be necessary.
process_params <- function(params_valid) {
  if (!is.null(params_valid$filepath_tweets)) {
    tweets <-
      # try(rtweet::read_twitter_csv(params_valid$filepath_tweets),
      #     silent = TRUE)
      try(readRDS(params_valid$filepath_tweets),
          silent = TRUE)
    if (inherits(tweets, "try-error")) {
      stop(
        sprintf(
          "Could not retrieve tweets from `filepath_tweets` %s.",
          params_valid$filepath_tweets
        ),
        call. = FALSE
      )
    }

    tweets <-
      tweets %>%
      filter(name %in% c(params_valid$names_main))

    # TODO: Need to validate length of params_valid$names vs. screen params_valid$names,
    # that name_main is in params_valid$names, etc.
    # browser()
    if (is.null(params_valid$names)) {
      if (params_valid$augmented) {
        colname <- params_valid$augmented_colname
      } else {
        colname <- "screen_name"
      }
      params_valid$names <-
        tweets %>%
        distinct(!!rlang::sym(colname)) %>%
        arrange(!!rlang::sym(colname)) %>%
        pull(!!rlang::sym(colname))
    } else {
      # params_valid$names <- params_valid$names
    }
  } else if (params_valid$download) {
    if (is.null(screen_names)) {
      stop(sprintf("Please provide `screen_names`."), call. = FALSE)
    }

    if (method == "timeline") {
      tweets <- try(rtweet::get_timelines(params_valid$screen_names))
    } else if (method == "search") {
      tweets <- try(rtweet::search_tweets(params_valid$screen_names))
    }
    if (inherits(tweets, "try-error")) {
      stop(sprintf("Could not download tweets."), call. = FALSE)
    }
    params_valid$names <- params_valid$names
  } else {
    stop("An unexpected combination of inputs was provided.", call. = FALSE)
  }
  out <- c(list(tweets = tweets, names = params_valid$names), params)
  out
}
params_proc <- process_params(params_valid)
```





```{r tweets_init}
# Process. ----
# `time` is replicated from https://buzzfeednews.github.io/2018-01-trump-twitter-wars/.
# Other columns are replicated from https://juliasilge.com/blog/ten-thousand-tweets/.
cols_keep <-
  c(
    "name",
    "status_id",
    "created_at",
    "user_id",
    "screen_name",
    "text",
    "display_text_width",
    "reply_to_status_id",
    "is_quote",
    "is_retweet",
    "favorite_count",
    "retweet_count",
    "hashtags",
    "symbols",
    "urls_url",
    "urls_expanded_url",
    "media_expanded_url",
    "ext_media_expanded_url"
  )

# See https://github.com/mkearney/rstudioconf_tweets/blob/master/README.Rmd.
round_time <- function(x, sec) {
  as.POSIXct(hms::hms(as.numeric(x) %/% sec * sec))
}

tweets <-
  params_proc$tweets %>%
  select(one_of(c(cols_keep))) %>%
  mutate(timestamp = lubridate::ymd_hms(created_at)) %>%
  mutate(timestamp = lubridate::with_tz(timestamp, "America/Chicago")) %>%
  mutate(time = round_time(timestamp, 60 * 60)) %>%
  mutate(time = lubridate::hour(timestamp) + lubridate::minute(timestamp) / 60) %>%
  mutate(text_plain = rtweet::plain_tweets(text))
```





```{r timefilter}
# Modified from a SO answer.
compute_elapsed_time <- function(date_start, date_end, type) {
  if (type == "years" | type == "months") {
    date_start <- as.POSIXlt(date_start)
    date_end <- as.POSIXlt(date_end)
    if (type == "years") {
      out <- (date_end$year - date_start$year) - 1
    } else if (type == "months") {
      out <- 12 * (date_end$year - date_start$year) + (date_end$mon - date_start$mon) - 1
    }
  } else if (type == "days" | type == "hours") {
    out <-
      (difftime(date_end, date_start, units = type) - 1) %>%
      round(0) %>%
      as.numeric()
  }
  out
}

compute_tweet_timefilter <- function(data, colnames_group = "name") {
  data_proc <-
    data %>%
    group_by(!!!rlang::syms(colnames_group)) %>%
    arrange(timestamp) %>%
    mutate(date_start = first(timestamp),
           date_end = last(timestamp)) %>%
    slice(1) %>%
    ungroup() %>%
    select(name, date_start, date_end) %>%
    mutate(
      yyyy_elapsed = compute_elapsed_time(date_start, date_end, "years"),
      mm_elapsed = compute_elapsed_time(date_start, date_end, "months"),
      dd_elapsed = compute_elapsed_time(date_start, date_end, "days"),
      hh_elapsed = compute_elapsed_time(date_start, date_end, "hours")
    )
  out <-
    list(data = data_proc, date_start = max(data_proc$date_start), date_end = min(data_proc$date_end))
  out
}

tweet_timefilter <-
  tweets %>%
  compute_tweet_timefilter()
```





```{r timefilter_dep, eval = (params_proc$trim_time), results = "asis", fig.show = "asis"}
# tweets ----
# This is "original" processing needed to trim tweets appropriately/dynamically
# given an unknown data set.
trim_tweets_bytime <-
  function(data,
           colname_time = "timestamp",
           start = NULL,
           end = NULL) {
    # browser()
    out <-
      data %>%
      filter(!!rlang::sym(colname_time) < end, !!rlang::sym(colname_time) >= start)
    out
  }

# cat(
#   sprintf(
#     "The tweets were trimmed in order to align the dates of the
#     most recent first tweet and the oldest last tweet."
#   )
# )
if(params_proc$trim_time) {
  tweets <-
    tweets %>%
    trim_tweets_bytime(start = tweet_timefilter$date_start, end = tweet_timefilter$date_end)
}
```


# Tweet Volume

How often does each Twitter handle tweet?
Does the volume of tweets look different for
temporal periods?


```{r viz_bytime_create}
# Inspired by https://juliasilge.com/blog/ten-thousand-tweets/ here.
# cnt_bytime ----
# NOTE: This function takes a gg object as an input
# (so it should/can be preceded by a pipe, and, if followed by other ggplot2 commands,
#  succedeeded by a `+`.)
add_viz_bytime_elements <-
  function(viz,
           geom = c("bar", "hist"),
           colors = params_proc$colors_main) {
    geom <- match.arg(geom)
    viz_labs <-
      labs(x = NULL, y = NULL, title = "Count of Tweets Over Time")
    viz_theme <-
      temisc::theme_te_2_facet() +
      theme(panel.grid.major.x = element_blank()) +
      # theme(legend.position = "bottom", legend.title = element_blank())
      theme(legend.position = "none")

    if (geom == "bar") {
      viz <-
        viz +
        geom_bar(aes(y = ..count.., fill = name))
    } else if (geom == "hist") {
      viz <-
        viz +
        geom_histogram(aes(y = ..count.., fill = name), bins = 30)
    }
    viz <-
      viz +
      scale_fill_manual(values = colors) +
      facet_wrap( ~ name, ncol = 1, strip.position = "right") +
      viz_labs +
      viz_theme
    viz
  }

lab_subtitle <-
  paste0(
    "From ",
    strftime(tweet_timefilter$date_start, "%Y-%m-%d"),
    " to ",
    strftime(tweet_timefilter$date_end, "%Y-%m-%d")
  )

viz_bytime_all <-
  tweets %>%
  ggplot(aes(x = timestamp)) %>%
  add_viz_bytime_elements(geom = "hist") +
  labs(subtitle = lab_subtitle)
# viz_bytime_all

viz_bytime_yyyy <-
  tweets %>%
  ggplot(aes(x = lubridate::year(timestamp))) %>%
  add_viz_bytime_elements(geom = "bar") +
  labs(subtitle = "By Year")
# viz_bytime_yyyy


viz_bytime_mm <-
  tweets %>%
  ggplot(aes(x = lubridate::month(timestamp))) %>%
  add_viz_bytime_elements(geom = "bar") +
  labs(subtitle = "By Month")
# viz_bytime_mm

viz_bytime_wday <-
  tweets %>%
  ggplot(aes(x = lubridate::wday(timestamp, label = TRUE))) %>%
  add_viz_bytime_elements(geom = "bar") +
  labs(subtitle = "By Day of Week")
# viz_bytime_wday

viz_bytime_hh <-
  tweets %>%
  ggplot(aes(x = name, y = time, fill = name)) +
  scale_y_continuous(
    limits = c(1, 24),
    breaks = c(6, 12, 18),
    labels = c("6am", "Noon", "6pm")
  ) +
  scale_fill_manual(values = params_proc$colors_main, guide = FALSE) +
  geom_violin(size = 0, alpha = 0.7) +
  geom_hline(yintercept = seq(3, 24, by = 3),
             color = "gray",
             size = 0.1) +
  labs(x = NULL, y = NULL, title = "Distribution of Tweets By Time of Day") +
  temisc::theme_te_2_dx() +
  theme(panel.grid = element_blank()) +
  coord_flip()
# viz_bytime_hh
```





```{r }
# + viz_bytime_all_show, eval = min(tweet_timefilter$data$dd_elapsed) >= params_proc$wday_cnt_min), results = "asis", fig.show = "asis"
viz_bytime_all
```





```{r }
# + viz_bytime_yyyy_show, eval = min(tweet_timefilter$data$yyyy_elapsed) >= params_proc$yyyy_cnt_min), results = "asis", fig.show = "asis"
viz_bytime_yyyy
```





```{r }
# + viz_bytime_mm_show, eval = min(tweet_timefilter$data$mm_elapsed) >= params_proc$mm_cnt_min), results = "asis", fig.show = "asis"
viz_bytime_mm
```





```{r }
# + viz_bytime_wday_show, eval = min(tweet_timefilter$data$dd_elapsed) >= params_proc$wday_cnt_min), results = "asis", fig.show = "asis"
viz_bytime_wday
```





```{r }
# + viz_bytime_hh_show, eval = min(tweet_timefilter$data$hh_elapsed) >= params_proc$hh_cnt_min), results = "asis", fig.show = "asis"
viz_bytime_hh
```


# Tweet Behavior

What proportion of tweets include more than just plain text
(e.g. hashtags or links)?
What proportion are **not** undirected or self-authored tweets
(i.e. RTs, replies, or quotes)?



```{r bykind_create}
# Inspired by https://juliasilge.com/blog/ten-thousand-tweets/ here.
# tweets_bykind ----
add_tweet_kind_data <- function(data) {
  out <-
    data %>%
    mutate(
      hashtag = if_else(!is.na(hashtags), 1, 0),
      link = if_else(!is.na(media_expanded_url) & !is.na(ext_media_expanded_url), 1, 0),
      rt = if_else(is_retweet, 1, 0),
      quote = if_else(is_quote, 1, 0),
      reply = if_else(!is.na(reply_to_status_id), 1, 0)
    ) %>%
    mutate(type =
             case_when(
               rt == TRUE ~ "RT",
               reply == TRUE ~ "reply",
               quote == TRUE ~ "quote",
               TRUE ~ "original"
             ))
  out
}

compute_pct <- function(x,
                        value = 1,
                        digits_round = 4) {
  # round(sum(x == value) / sum(!is.na(x)), digits_round)
  sum(x == value) / sum(!is.na(x))
}

tweets <-
  tweets %>%
  add_tweet_kind_data()

kinds <- c(params_proc$kinds_features, params_proc$kinds_types)
cols_summarize <- kinds
tweets_byname_bykind_summary_tidy <-
  tweets %>%
  group_by(name) %>%
  summarize_at(vars(c(cols_summarize)), funs(compute_pct(.))) %>%
  ungroup() %>%
  tidyr::gather(kind, value, -name)
tweets_byname_bykind_summary_tidy
```





```{r viz_bykind_create}
visualize_byname_bykind <-
  function(data,
           geom = c("col", "lollipop"),
           colors = params_proc$colors_main,
           data_labels = FALSE) {
    geom <- match.arg(geom)

    viz_labs <-
      labs(x = NULL, y = NULL, title = "% of Tweets")

    viz_theme <-
      temisc::theme_te_2_facet() +
      theme(panel.grid.major.x = element_blank()) +
      theme(legend.position = "none")

    if (geom == "col") {
      viz <-
        data %>%
        ggplot(aes(x = name, y = value, fill = name)) +
        geom_col(position = "dodge") +
        scale_fill_manual(values = colors)
    } else if (geom == "lollipop") {
      viz <-
        data %>%
        ggplot(aes(x = name, y = value, color = name)) +
        ggalt::geom_lollipop(size = 2, point.size = 4) +
        scale_color_manual(values = colors)
    }

    if (data_labels) {
      viz <-
        viz +
        geom_text(aes(group = name,
                      label = sprintf("%.0f %%", 100 * value)),
                  position = position_dodge(width = 1))
    }

    viz <-
      viz +
      scale_y_continuous(labels = scales::percent_format()) +
      facet_wrap( ~ kind, scales = "free") +
      viz_labs +
      viz_theme
    viz
  }

viz_byname_bytype <-
  tweets_byname_bykind_summary_tidy %>%
  filter(kind %in% params_proc$kinds_types) %>%
  mutate(kind = str_to_title(kind)) %>%
  visualize_byname_bykind() +
  labs(subtitle = "Types")
# viz_byname_bytype

viz_byname_byfeature <-
  tweets_byname_bykind_summary_tidy %>%
  filter(kind %in% params_proc$kinds_features) %>%
  mutate(kind = str_to_title(kind)) %>%
  visualize_byname_bykind() +
  labs(subtitle = "Features")
# viz_byname_byfeature
```





```{r viz_byname_bykind_show, results = "asis", fig.show = "asis"}
viz_byname_bytype
viz_byname_byfeature
```





```{r viz_byname_bytype_temporal_create}
# Inspired by https://juliasilge.com/blog/ten-thousand-tweets/ here.
viz_byname_bytype_temporal <-
  tweets %>%
  ggplot(aes(x = timestamp, fill = type)) +
  geom_histogram(position = "fill", bins = 30) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_manual(values = viridis::viridis(n = length(params_proc$kinds_types) + 1)) +
  facet_wrap( ~ name,
              ncol = 1,
              scales = "free",
              strip.position = "right") +
  labs(x = NULL, y = NULL) +
  labs(title = "Distribution of Tweets by Type Over Time") +
  temisc::theme_te_2_facet() +
  theme(panel.grid.major.x = element_blank()) +
  theme(legend.position = "bottom", legend.title = element_blank())
# viz_byname_bytype_temporal
```





```{r viz_byname_bytype_temporal_show, eval = (min(tweet_timefilter$data$dd_elapsed) >= params_proc$dd_cnt_min), results = "asis", fig.show = "asis"}
viz_byname_bytype_temporal
```


# Tweet Content

How long are the tweets?


```{r viz_byname_chars_cnts_create}
# Note that there are some tweets above 140 characters.
# Inspired by https://juliasilge.com/blog/ten-thousand-tweets/ here.
viz_byname_chars_cnt <-
  tweets %>%
  # mutate(chars_cnt = str_length(text)) %>%
  mutate(chars_cnt = display_text_width) %>%
  ggplot(aes(x = chars_cnt)) +
  geom_density(aes(fill = name)) +
  scale_fill_manual(values = params_proc$colors_main) +
  scale_x_continuous(limits = c(0, 200)) +
  facet_wrap( ~ name, scales = "free") +
  labs(x = NULL, y = NULL) +
  labs(title = "Distribution of Characters Per Tweet") +
  temisc::theme_te_2_facet() +
  theme(panel.grid.major.x = element_blank()) +
  theme(legend.position = "none")
# viz_byname_chars_cnt
```





```{r viz_byname_chars_cnt_show, results = "asis", fig.show = "asis"}
viz_byname_chars_cnt
```


## Word Frequency and Usage

Which words are used most frequently?


```{r tweets_tidy}
# Inspired by https://www.tidytextmining.com/twitter.html here.
# Bigrams inspired by https://www.tidytextmining.com/ngrams.html and
# https://buzzfeednews.github.io/2018-01-trump-twitter-wars/.
# tweets_tidy ----
# library("tidytext")
rgx_unnest <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
rgx_pattern <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
rgx_ignore_custom <- "^[0-9f]+$"

tweets_tidy <-
  tweets %>%
  # filter(!str_detect(text, "^(RT|@)")) %>%
  filter(is_retweet == FALSE) %>%
  mutate(text = text_plain) %>%
  mutate(text = str_replace_all(text, rgx_pattern, "")) %>%
  tidytext::unnest_tokens(word, text, token = "regex", pattern = rgx_unnest) %>%
  anti_join(tidytext::stop_words, by = "word") %>%
  filter(!str_detect(word, rgx_ignore_custom)) %>%
  filter(str_detect(word, "[a-z]"))
tweets_tidy %>% select(word, name, status_id) %>% count(word, sort = TRUE)

tweets_tidy_bigrams <-
  tweets %>%
  mutate(text = text_plain) %>%
  filter(is_retweet == FALSE) %>%
  tidytext::unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
  anti_join(tidytext::stop_words, by = c("first" = "word")) %>%
  anti_join(tidytext::stop_words, by = c("second" = "word")) %>%
  filter(!str_detect(first, rgx_ignore_custom), !str_detect(second, rgx_ignore_custom)) %>%
  filter(str_detect(first, "[a-z]"), str_detect(second, "[a-z]"))
tweets_tidy_bigrams %>% select(bigram, name, status_id) %>% count(bigram, sort = TRUE)
```




```{r viz_byname_cnt_create}
# Inspired by https://github.com/dgrtwo/dgrtwo.github.com/blob/master/_R/2016-08-09-trump-tweets.Rmd here.
# TODO: Improve this.
visualize_byname_cnt <-
  function(data,
           num_top = 20,
           facet = FALSE,
           colors = params_proc$pal_main) {
    # browser()
    if (facet) {
      data_proc <-
        data %>%
        count(name, word, sort = TRUE) %>%
        group_by(name) %>%
        filter(row_number(desc(n)) <= num_top) %>%
        mutate(word = reorder(word, n)) %>%
        # arrange(desc(n)) %>%
        # mutate(word = forcats::fct_inorder(word)) %>%
        ungroup()
    } else {
      data_proc <-
        data %>%
        count(word, sort = TRUE) %>%
        filter(row_number(desc(n)) <= num_top) %>%
        mutate(word = reorder(word, n))
    }

    # browser()
    viz <-
      data_proc %>%
      ggplot(aes(x = word, y = n)) +
      geom_bar()

    if (facet) {
      viz <-
        viz +
        geom_bar(stat = "identity", aes(color = name)) +
        ggalt::geom_lollipop(aes(color = name), size = 2, point.size = 2) +
        scale_fill_manual(values = colors) +
        facet_wrap( ~ name, scales = "free") +
        temisc::theme_te_2_facet() +
        labs(subtitle = "By Name")
    } else {
      viz <-
        data_proc %>%
        ggplot(aes(x = word, y = n)) +
        # geom_bar(stat = "identity") +
        ggalt::geom_lollipop(size = 2, point.size = 2) +
        temisc::theme_te_2()
    }
    # browser()
    viz <-
      viz +
      labs(x = NULL, y = NULL) +
      labs(title = "Count of Words") +
      theme(panel.grid.major.y = element_blank()) +
      coord_flip()
    viz
  }
viz_byname_cnt <-
  tweets_tidy %>%
  visualize_byname_cnt()
# viz_byname_cnt

# viz_byname_cnt_byname <-
#   tweets_tidy %>%
#   visualize_byname_cnt(facet = TRUE)
# viz_byname_cnt_byname
```





```{r viz_byname_cnt_show, results = "asis", fig.show = "asis"}
viz_byname_cnt
# viz_byname_cnt_byname
```





```{r ngrams_cnt_create}
unigrams_cnt <-
  tweets_tidy %>%
  # group_by(name) %>%
  count(name, sort = TRUE)
unigrams_cnt

bigrams_cnt <-
  tweets_tidy_bigrams %>%
  count(name, sort = TRUE)
  # count(bigram, sort = TRUE)
bigrams_cnt

# freqs ----
unigrams_byname_freqs <-
  tweets_tidy %>%
  count(name, word, sort = TRUE) %>%
  left_join(unigrams_cnt %>% rename(total = n), by = "name") %>%
  mutate(freq = n / total)
unigrams_byname_freqs

bigrams_byname_freqs <-
  tweets_tidy_bigrams %>%
  count(name, bigram, sort = TRUE) %>%
  left_join(bigrams_cnt %>% rename(total = n), by = "name") %>%
  mutate(freq = n / total)
bigrams_byname_freqs

# Debugging...
unigrams_byname_freqs %>%
  inner_join(unigrams_byname_freqs, by = "word", suffix = c("_x", "_y")) %>%
  filter(name_x != name_y)
bigrams_byname_freqs %>%
  inner_join(bigrams_byname_freqs, by = "bigram", suffix = c("_x", "_y")) %>%
  filter(name_x != name_y)

num_top_bigram_freq <- floor(12 / length(params_proc$names_main))
bigrams_byname_freqs_viz <-
  bigrams_byname_freqs %>%
  group_by(name) %>%
  mutate(rank = row_number(desc(freq))) %>%
  filter(rank <= num_top_bigram_freq) %>%
  ungroup() %>%
  arrange(name) %>%
  mutate(bigram = str_replace_all(bigram, " ", "\n")) %>%
  mutate(bigram = forcats::fct_reorder(factor(bigram), freq))
bigrams_byname_freqs_viz
```





```{r viz_ngrams_byname_freqs_create}
viz_bigrams_byname_freqs <-
  bigrams_byname_freqs_viz %>%
  ggplot(aes(x = name, y = bigram, color = name, size = freq)) +
  geom_point() +
  scale_y_discrete(position = "right") +
  scale_color_manual(values = params_proc$colors_main) +
  scale_size_area(max_size = 25) +
  labs(x = NULL, y = NULL) +
  labs(title = "Top Bigrams") +
  temisc::theme_te_2(base_family = "") +
  theme(legend.position = "none") +
  coord_flip()
viz_bigrams_byname_freqs
```





```{r viz_ngrams_byname_freqs_show, results = "asis", fig.show = "asis"}
viz_bigrams_byname_freqs
```





```{r viz_ngrams_byname_freqs_wordcloud_create}
# TODO: Word cloud (bigram only)
visualize_bigrams_byname_freqs_wordcloud <-
  function(name_filter, data = bigrams_byname_freqs) {
    data_proc <-
      data %>%
      filter(name == name_filter)

    color <-
      params_proc$colors_main[names(params_proc$colors_main) == name_filter]
    out <-
      wordcloud::wordcloud(
        word = data_proc$bigram,
        freq = data_proc$n,
        max.words = 50,
        random.order = FALSE,
        colors = color
      )
    out
  }
num_par_row <- ceiling(length(params_proc$names_main) / 3)
num_par_col <- min(length(params_proc$names_main), 3)

# viz_bigrams_byname_freqs_wordclouds <-
#   lapply(params_proc$names_main, visualize_bigrams_byname_freqs_wordcloud)
```





```{r viz_ngrams_byname_freqs_wordcloud_show, results = "asis", fig.show = "asis"}
# par(mfrow = c(num_par_row, num_par_col))
# lapply(params_proc$names_main, visualize_bigrams_byname_freqs_wordcloud)
# par(mfrow = c(1, 1))
```





```{r names_grid}
names_distinct <- unigrams_byname_freqs %>% distinct(name) %>% pull(name)
names_grid <-
  bind_cols(x = names_distinct, y = names_distinct) %>%
  tidyr::complete(x, y) %>%
  filter(x != y) %>%
  mutate(xy = str_c(x, "_", y)) %>%
  mutate(i = row_number())
xy_names <- names_grid %>% pull(xy)

filter_xy_names <- function(xy_names, i) {
  xy_i <- xy_names[i]
  xy_i_row <- names_grid %>% filter(xy == xy_i)
  x_i <- xy_i_row %>% pull(x)
  y_i <- xy_i_row %>% pull(y)
  out <- list(x = x_i, y = y_i, xy = xy_i)
  out
}

preprocess_xy_data <- function(data, xy_info) {
  out <-
    data %>%
    filter(name %in% c(xy_info$x, xy_info$y))
  out
}

postprocess_xy_data <- function(data, xy_info) {
  # browser()
  out <-
    data %>%
    mutate(name_x = xy_info$x, name_y = xy_info$y) %>%
    mutate(name_xy = paste0(name_x, "_", name_y))
  if (length(setdiff(c(xy_info$x, xy_info$y), names(data))) == 0) {
    out <-
      out %>%
      rename(x = !!rlang::sym(xy_info$x),
             y = !!rlang::sym(xy_info$y)) %>%
      select(name_x, name_y, name_xy, x, y, everything())
  } else {
    out <- out %>% select(name_x, name_y, name_xy, everything())
  }
  out
}

compute_unigrams_freqs <- function(data) {
  out <-
    data %>%
    select(name, word, freq) %>%
    tidyr::spread(name, freq)
  out
}

# TODO: Figure out how to use `purrr::map()` here.
wrapper_func <- function(xy_names, data, func) {
  i <- 1
  while (i < length(xy_names)) {
    xy_i_info <- filter_xy_names(xy_names, i)
    data_i_preproc <- preprocess_xy_data(data, xy_i_info)
    # browser()
    data_i_proc <- do.call(func, list(data_i_preproc))
    # browser()
    data_i_postproc <- postprocess_xy_data(data_i_proc, xy_i_info)
    if (i == 1) {
      out <- data_i_postproc
    } else {
      out <- bind_rows(out, data_i_postproc)
    }
    i <- i + 1
  }
  out
}
```





```{r ngrams_freqs_wide_create}
unigrams_freqs_wide <-
  wrapper_func(xy_names = xy_names,
               data = unigrams_byname_freqs,
               func = compute_unigrams_freqs)
unigrams_freqs_wide

# Debugging...
unigrams_freqs_wide %>%
  count(name_xy, name_x) %>%
  group_by(name_x) %>%
  filter(row_number(desc(n)) <= 2) %>%
  ungroup() %>%
  distinct(n, .keep_all = TRUE)

# TODO: Need to make this more dynamic (with if statements)...
# unigrams_freqs_wide_top <-
#   unigrams_freqs_wide %>%
#   count(name_xy, name_x) %>%
#   group_by(name_x) %>%
#   filter(row_number(desc(n)) == 1)

unigrams_freqs_wide_viz <-
  unigrams_freqs_wide %>%
  # semi_join(unigrams_freqs_wide_top, by = "name_xy") %>%
  filter(name_x == name_main) %>%
  mutate(name_xy = paste0(name_x, " vs. ", name_y))
```





```{r viz_ngrams_freqs_create}
viz_unigrams_freqs <-
  unigrams_freqs_wide_viz %>%
  ggplot(aes(x = x, y = y)) +
  # geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word, size = x + y), check_overlap = TRUE) +
  scale_x_log10(labels = scales::percent_format()) +
  scale_y_log10(labels = scales::percent_format()) +
  geom_abline(color = "red") +
  facet_wrap( ~ name_xy, scales = "free") +
  labs(x = NULL, y = NULL) +
  labs(title = "Relative Word Frequency") +
  temisc::theme_te_2_facet(base_family = "") +
  theme(legend.position = "none")
viz_unigrams_freqs
```





```{r ngrams_freqs_show, results = "asis", fig.show = "asis"}
viz_unigrams_freqs
```


Which words are most likely to be used by one name compared to the other?


```{r ngrams_ratios_wide}
# unigrams_ratios ----
# NOTE: Filter out replies because they would make up a disproportional share of the
# top results.
# NOTE: Modified original code a bit because it doesn't really
compute_logratio <- function(data) {
  out <-
    data %>%
    filter(is.na(reply_to_status_id)) %>%
    count(word, name) %>%
    filter(n >= 10) %>%
    tidyr::spread(name, n, fill = 0)

  nms <- names(out)
  out <-
    out %>%
    setNames(c("word", "x", "y")) %>%
    mutate_if(is.numeric, funs((. + 1) / sum(. + 1))) %>%
    mutate(logratio = log(x / y))

  x_i <- nms[2]
  y_i <- nms[3]
  out <-
    out %>%
    setNames(c("word", x_i, y_i, "logratio")) %>%
    arrange(desc(logratio))
  out
}

unigrams_ratios_wide <-
  wrapper_func(xy_names = xy_names,
               data = tweets_tidy,
               func = compute_logratio)
unigrams_ratios_wide %>% count(name_xy, sort = TRUE)

# These words are the most and least likely to be tweeted by either name.
unigrams_ratios_wide %>% group_by(name_xy) %>% arrange(abs(logratio))
unigrams_ratios_wide %>% group_by(name_xy) %>% arrange(desc(abs(logratio)))

unigrams_ratios_wide_viz <-
  unigrams_ratios_wide %>%
  mutate(logratio_direction = ifelse(logratio < 0, TRUE, FALSE)) %>%
  group_by(name_xy, logratio_direction) %>%
  # top_n(15, abs(logratio)) %>%
  arrange(name_xy, desc(abs(logratio))) %>%
  slice(1:5) %>%
  ungroup() %>%
  # mutate(word = reorder(word, logratio)) %>%
  filter(name_x == params_proc$name_main) %>%
  mutate(logratio_lab = ifelse(logratio_direction == TRUE, name_x, "other")) %>%
  mutate(word = reorder(word, -logratio)) %>%
  mutate(name_xy = paste0(name_x, " vs. ", name_y))
```





```{r ngrams_ratios_create}
# pal_rdbu <- RColorBrewer::brewer.params_proc$pal_main(n = 3, name = "RdBu")
# pal_rdbu_red <- pal_rdbu[1]
# pal_rdbu_blue <- pal_rdbu[length(pal_rdbu)]
# pal_rdbu_rb <- c(pal_rdbu_red, pal_rdbu_blue)
color_main_inv <- temisc::get_color_hex_inverse(params_proc$color_main)
colors_dual <- c(params_proc$color_main, color_main_inv)
# names(colors_dual) <- NULL
# names(colors_dual) <- c(params_proc$color_main, "other")

viz_unigrams_ratios <-
  unigrams_ratios_wide_viz %>%
  ggplot(aes(x = word, y = logratio, fill = logratio_lab)) +
  geom_col() +
  facet_wrap(~ name_xy, scales = "free") +
  # scale_fill_manual(values = c("red", "cyan")) +
  # scale_fill_manual(values = pal_rdbu_rb) +
  scale_fill_manual(values = colors_dual) +
  # scale_fill_manual(values = colors_dual) +
  coord_flip() +
  labs(x = NULL, y = "log odds ratio") +
  labs(title = "Most Unique Words") +
  temisc::theme_te_2_facet() +
  # theme(legend.position = "none") +
  theme(legend.position = "bottom", legend.title = element_blank())
viz_unigrams_ratios
```





```{r ngrams_ratios_show, results = "asis", fig.show = "asis"}
viz_unigrams_ratios
```





```{r change_byname_bytime}
# Inspired by https://www.tidytextmining.com/twitter.html here.
# TODO: change_byname_bytime? ----
```





```{r pop_byname_byfeature}
# Inspired by https://www.tidytextmining.com/twitter.html here.
# TODO: pop_byname_byfeature? ----
```


# Sentiment Analysis

What is the sentiment (i.e. "tone") of the tweets?


```{r sents_diffs}
# sents_diffs ----
bing <-
  tidytext::get_sentiments(lexicon = "bing") %>%
  select(word, sentiment)

afinn <-
  tidytext::get_sentiments(lexicon = "afinn") %>%
  select(word, sentiment = score)

unigrams_cnt_byname_bytweet <-
  tweets_tidy %>%
  group_by(name) %>%
  mutate(total_words = n()) %>%
  ungroup() %>%
  distinct(status_id, name, total_words)
unigrams_cnt_byname_bytweet

summarize_sent_byname <-
  function(sents,
           tweets = tweets_tidy,
           unigrams = unigrams_cnt_byname_bytweet) {
    out <-
      tweets %>%
      inner_join(sents, by = "word") %>%
      count(status_id, sentiment) %>%
      tidyr::complete(sentiment, status_id, fill = list(n = 0)) %>%
      inner_join(unigrams, by = "status_id") %>%
      group_by(name, sentiment, total_words) %>%
      summarize(words = sum(n)) %>%
      ungroup()
    out
  }

sents_bing_byname <-
  bing %>%
  summarize_sent_byname()
sents_bing_byname

sents_afinn_byname_0 <-
  afinn %>%
  # mutate(sentiment = sentiment / 5) %>%
  summarize_sent_byname()
sents_afinn_byname_0

sents_afinn_byname_chars <-
  sents_afinn_byname_0 %>%
  mutate(sentiment = if_else(sentiment <= 0, "negative", "positive")) %>%
  group_by(name, sentiment) %>%
  summarize(total_words = first(total_words), words = sum(words)) %>%
  ungroup()
sents_afinn_byname_chars

sents_afinn_byname_nums <-
  sents_afinn_byname_0 %>%
  group_by(name) %>%
  summarize(sentiment = sum(sentiment * words) / sum(words), total_words = first(total_words)) %>%
  ungroup()
sents_afinn_byname_nums
```





```{r sents_diffs_poisson}
compute_sentdiffs_poisson <- function(data) {
  out <-
    data %>%
    group_by(sentiment) %>%
    do(broom::tidy(poisson.test(.$words, .$total_words))) %>%
    ungroup()
  out
}

sents_bing_diffs_poisson <-
  wrapper_func(xy_names = xy_names,
               data = sents_bing_byname,
               func = compute_sentdiffs_poisson)
sents_bing_diffs_poisson %>% filter(name_x == params_proc$name_main)

sents_afinn_diffs_poisson <-
  wrapper_func(xy_names = xy_names,
               data = sents_afinn_byname_chars,
               func = compute_sentdiffs_poisson)
sents_afinn_diffs_poisson

prepare_sents_diffs_poisson <- function(data) {
  out <-
    data %>%
    filter(name_x == params_proc$name_main) %>%
    mutate(name_xy = paste0(name_x, " vs. ", name_y)) %>%
    mutate(sentiment = reorder(sentiment, estimate)) %>%
    mutate_at(vars(estimate, conf.low, conf.high), funs(. - 1))
  out

}

sents_bing_diffs_poisson_viz <-
  sents_bing_diffs_poisson %>%
  prepare_sents_diffs_poisson()

sents_afinn_diffs_poisson_viz <-
  sents_afinn_diffs_poisson %>%
  prepare_sents_diffs_poisson()
```





```{r viz_sents_diffs_poisson_create}
visualize_sents_diffs_poission <- function(data, lab_lexicon) {
  out <-
    data %>%
    ggplot(aes(x = estimate, y = sentiment)) +
    geom_point(size = 2) +
    # geom_point(aes(color = name_x), size = 2) +
    # scale_color_manual(values = params_proc$pal_main) +
    geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), linetype = "solid") +
    geom_vline(aes(xintercept = 0), size = 2) +
    facet_wrap(~ name_xy, scales = "free") +
    scale_x_continuous(labels = scales::percent_format()) +
    labs(x = NULL, y = NULL) +
    labs(title = "Likelihood of Differences Among Sentiments",
         caption = paste0(lab_lexicon, " lexicon used for classification of words.\n",
                          "Estimate and 95% confidence intervals of poisson test shown.")) +
    temisc::theme_te_2_facet() +
    theme(legend.position = "none")

  out
}

viz_sents_bing_diffs_poisson <-
  sents_bing_diffs_poisson_viz %>%
  visualize_sents_diffs_poission(lab_lexicon = "Bing")
viz_sents_bing_diffs_poisson
viz_sents_afinn_diffs_poisson <-
  sents_afinn_diffs_poisson_viz %>%
  visualize_sents_diffs_poission(lab_lexicon = "AFINN")
viz_sents_afinn_diffs_poisson
```





```{r viz_sents_diffs_poisson_show, results =  "asis", fig.show = "asis"}
viz_sents_bing_diffs_poisson
```





```{r sents_ratios_create}
num_top_sents_ratio <- floor(12 / length(params_proc$names_main))

create_sents_ratios_wide <-
  function(sents,
           unigrams_ratios = unigrams_ratios_wide,
           num_top = num_top_sents_ratio) {

    # Filter early so that reorder works properly.
    data_proc <-
      unigrams_ratios %>%
      inner_join(sents, by = "word") %>%
      filter(name_x == params_proc$name_main) %>%
      group_by(word, sentiment) %>%
      mutate(logratio = mean(logratio)) %>%
      ungroup() %>%
      mutate(sentiment = reorder(sentiment, -logratio),
             word = reorder(word, -logratio)) %>%
      group_by(name_xy, sentiment) %>%
      # group_by(sentiment) %>%
      mutate(rank = row_number(desc(logratio))) %>%
      filter(rank <= num_top |
               rank >= (max(rank) - num_top)) %>%
      ungroup()


    out <-
      data_proc %>%
      mutate(logratio_direction = ifelse(logratio < 0, TRUE, FALSE)) %>%
      mutate(logratio_lab = ifelse(logratio_direction == TRUE, params_proc$name_main, "other")) %>%
      mutate(name_xy = paste0(name_x, " vs. ", name_y))
  }
sents_bing_ratios_wide_viz <-
  bing %>%
  create_sents_ratios_wide()

sents_afinn_ratios_wide_viz <-
  afinn %>%
  mutate(sentiment = if_else(sentiment <= 0, "negative", "positive")) %>%
  create_sents_ratios_wide()
```





```{r viz_sents_ratios_create}
viz_sents_bing_ratios <-
  sents_bing_ratios_wide_viz %>%
  ggplot(aes(x = word, y = logratio, fill = logratio_lab)) +
  geom_bar(stat = "identity") +
  geom_hline(aes(yintercept = 0)) +
  scale_fill_manual(values = colors_dual) +
  facet_wrap( ~ sentiment, scales = "free") +
  # facet_wrap(name_xy ~ sentiment, scales = "free") +
  # facet_grid(name_xy ~ sentiment) +
  # facet_grid(sentiment ~ name_xy) +
  labs(x = NULL, y = "Log Odds Ratio") +
  labs(title = "Most Influential Words Contributing to Sentiment Differences") +
  temisc::theme_te_2_facet_dx() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme(legend.position = "bottom", legend.title = element_blank())
viz_sents_bing_ratios
```





```{r viz_sents_ratios_show, results = "asis", fig.show = "asis"}
viz_sents_bing_ratios
```





```{r ngrams_bytweet_corrs}
# Inspired by http://varianceexplained.org/r/seven-fav-packages/ and
# http://varianceexplained.org/r/stacksurveyr/ here.
# corrs ----

unigrams_byword_cnt <-
  tweets_tidy %>%
  count(word, sort = TRUE)

num_top_cnt <- 50
unigrams_byword_cnt_top <-
  unigrams_byword_cnt %>%
  mutate(rank = row_number(desc(n))) %>%
  filter(rank <= num_top_cnt)

unigrams_bytweet_corrs <-
  tweets_tidy %>%
  semi_join(unigrams_byword_cnt_top, by = "word") %>%
  widyr::pairwise_cor(word, status_id, sort = TRUE, upper = FALSE)
unigrams_bytweet_corrs

num_top_corr <- 50
# Arrange this way to see what the lowest correlations are.
unigrams_bytweet_corrs_viz <-
  unigrams_bytweet_corrs %>%
  mutate(rank = row_number(desc(correlation))) %>%
  filter(rank <= num_top_corr)
unigrams_bytweet_corrs_viz
unigrams_bytweet_corrs_viz %>% arrange(desc(rank))
```





```{r viz_ngrams_bytweet_corrs_create}
set.seed(42)
viz_unigrams_bytweet_corrs <-
  unigrams_bytweet_corrs_viz %>%
  igraph::graph_from_data_frame(vertices = unigrams_byword_cnt_top) %>%
  ggraph::ggraph(layout = "fr") +
  # ggraph::geom_edge_link(aes(edge_alpha = correlation / 1.01, edge_width = correlation / 1.01)) +
  ggraph::geom_edge_link(edge_width = 1) +
  ggraph::geom_node_point(aes(size = n), fill = "grey80", shape = 21) +
  ggraph::geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() +
  labs(title = "Network of Word Correlations",
       captions = "Correlations are not grouped by name here.") +
  theme(legend.position = "none")
viz_unigrams_bytweet_corrs
```





```{r viz_ngrams_bytweet_corrs_show, results = "asis", fig.show = "asis"}
viz_unigrams_bytweet_corrs
```

